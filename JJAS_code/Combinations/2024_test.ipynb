{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6560be90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from preprocessingtesting import DataPreprocessortesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ff85c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('best_models.pkl', 'rb') as f:\n",
    "    best_models = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1382c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_model = best_models['mam_Avg']\n",
    "second_model = best_models['Mar']\n",
    "third_model = best_models['Apr']\n",
    "fourth_model = best_models['May']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430d211f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('long_term_mean.pkl', 'rb') as f:\n",
    "    long_term_mean = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30658c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('long_term_anomalies.pkl', 'rb') as f:\n",
    "    long_term_anomalies = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1050666",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.load('weights.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2d9141",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sorted_correlation_data.pkl', 'rb') as f:\n",
    "    sorted_correlation_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fc7e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sorted_correlation_data['sorted_correlation_pearson']\n",
    "k = sorted_correlation_data['sorted_correlation_kendall']\n",
    "s = sorted_correlation_data['sorted_correlation_spearman']\n",
    "m = sorted_correlation_data['sorted_correlation_mutual']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f9857c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('best_scalers.pkl', 'rb') as f:\n",
    "    best_scalers = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d10890b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a9562b",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_scalar = best_scalers['mam_Avg']\n",
    "second_scalar = best_scalers['Mar']\n",
    "third_scalar = best_scalers['Apr']\n",
    "fourth_scalar = best_scalers['May']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb7e267",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class Predictor:\n",
    "    def __init__(self, nc_file1, nc_file2, weights,means,anomalies, correlation_file, best_model,best_scaler, top_n):\n",
    "        self.processor = DataPreprocessortesting(means,anomalies)\n",
    "        self.nc_file1 = nc_file1\n",
    "        self.nc_file2 = nc_file2\n",
    "        self.autoencoder_input_data = self.processor.execute_pipeline(nc_file1, nc_file2)\n",
    "        self.weights = weights\n",
    "        self.correlation_file = correlation_file\n",
    "        self.best_model = best_model\n",
    "        self.best_scaler = best_scaler\n",
    "        self.top_n = top_n\n",
    "        self.means = means\n",
    "        self.anomalies = anomalies\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    def convert_to_april_march(self,month_idx):\n",
    "        \n",
    "        return (month_idx-2) % 12 \n",
    "    \n",
    "    \n",
    "    def Tweights(self,weight):\n",
    "        threshold_values = []\n",
    "        for threshold_multiplier in np.arange(2, 1, -0.1):\n",
    "            num = []\n",
    "            for i in range(weight.shape[1]):\n",
    "                weight_mean = np.mean(weight[:, i])\n",
    "                weight_std = np.std(weight[:, i])\n",
    "                threshold_upper = weight_mean + threshold_multiplier * weight_std\n",
    "                threshold_lower = weight_mean - threshold_multiplier * weight_std\n",
    "                nodes_with_weight_above_upper_threshold = np.sum(weight[:, i] > threshold_upper)\n",
    "                nodes_with_weight_below_lower_threshold = np.sum(weight[:, i] < threshold_lower)\n",
    "                ten_percent_nodes = int(0.1 * weight.shape[0])\n",
    "\n",
    "                if (nodes_with_weight_above_upper_threshold + nodes_with_weight_below_lower_threshold) > ten_percent_nodes:\n",
    "                    num.append(nodes_with_weight_above_upper_threshold + nodes_with_weight_below_lower_threshold)\n",
    "                    #print(i, nodes_with_weight_above_upper_threshold, threshold_upper, nodes_with_weight_below_lower_threshold, threshold_lower)\n",
    "\n",
    "                    if len(num) == weight.shape[1]:\n",
    "                        threshold_values.append(threshold_multiplier)\n",
    "                        break\n",
    "            if len(num) == weight.shape[1]:\n",
    "                break\n",
    "\n",
    "        return threshold_values\n",
    "    \n",
    "        \n",
    "    def potential_pred(self,threshold_values,input_data,weights):\n",
    "        pred = np.zeros((65,input_data.shape[0]))\n",
    "        for i in range(weights.shape[1]):\n",
    "            weight_mean = np.mean(weights[:,i])\n",
    "            weight_std = np.std(weights[:,i])\n",
    "            threshold_upper = weight_mean + threshold_values[0] * weight_std\n",
    "            threshold_lower = weight_mean - threshold_values[0] * weight_std\n",
    "            nodes_with_weight_above_upper_threshold = np.sum(weights[:, i] > threshold_upper)\n",
    "            nodes_with_weight_below_lower_threshold = np.sum(weights[:, i] < threshold_lower)\n",
    "            ten_percent_nodes = int(0.1 * weights.shape[0])\n",
    "            if (nodes_with_weight_above_upper_threshold + nodes_with_weight_below_lower_threshold) > ten_percent_nodes:\n",
    "                for h in range(input_data.shape[0]): \n",
    "                    pred_i = 0\n",
    "                    for j in range(weights.shape[0]):\n",
    "                        weight_value = weights[j,i]\n",
    "                        if weight_value > threshold_upper or weight_value < threshold_lower:\n",
    "                            pp = np.sum(weight_value*input_data[h,j])\n",
    "                            pred_i += pp\n",
    "                            pred[i,h] = pred_i\n",
    "        return pred\n",
    "        \n",
    "    def get_pred_values(self):\n",
    "        # Function to retrieve predicted values\n",
    "        x_test = []\n",
    "        for idx, month_idx, value in self.correlation_file[:self.top_n]:\n",
    "            month_idx_april_march = self.convert_to_april_march(month_idx)\n",
    "            pred_value = self.pred[idx, month_idx_april_march]\n",
    "            x_test.append(pred_value)\n",
    "            #print(f\"Index: {idx}, Month: {month_idx_april_march}, Value: {pred_value}\")\n",
    "        return np.array(x_test).reshape(-1, 1)\n",
    "    \n",
    "    def predict(self):\n",
    "        th = self.Tweights(self.weights)\n",
    "        self.autoencoder_input_data = self.autoencoder_input_data.reshape(-1, self.weights.shape[0])\n",
    "        self.pred = self.potential_pred(th,self.autoencoder_input_data, self.weights)\n",
    "        x_test0 = self.get_pred_values( )\n",
    "        x_test0_reshaped = x_test0.reshape(1, -1)\n",
    "        X_new_scaled = self.best_scaler.transform(x_test0_reshaped)\n",
    "        predictions = self.best_model.predict(X_new_scaled)\n",
    "        return predictions\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab759f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "predictor_mam = Predictor('SLP_test.nc','Vwind_Test.nc', weights,long_term_mean,long_term_anomalies,k,first_model,first_scalar,10)\n",
    "prediction_MAMmean = predictor_mam.predict()\n",
    "prediction_MAMmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e39dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_march = Predictor('SLP_test.nc','Vwind_Test.nc', weights,long_term_mean,long_term_anomalies, p,second_model,second_scalar,15)\n",
    "prediction_march = predictor_march.predict()\n",
    "prediction_march"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce14600",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_april = Predictor('SLP_test.nc','Vwind_Test.nc', weights,long_term_mean,long_term_anomalies, p,third_model,third_scalar,10)\n",
    "prediction_april = predictor_april.predict()\n",
    "prediction_april"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117d7c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_may = Predictor('SLP_test.nc','Vwind_Test.nc',weights,long_term_mean,long_term_anomalies, s,fourth_model,fourth_scalar,20)\n",
    "prediction_may = predictor_may.predict()\n",
    "prediction_may"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a402592",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
