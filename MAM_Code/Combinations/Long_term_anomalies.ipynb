{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f763a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4 as nc\n",
    "from netCDF4 import num2date,date2index\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pickle\n",
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "from netCDF4 import num2date\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83eb7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DataPreprocessor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "\n",
    "    def preprocessing(self, nc_file):\n",
    "        data = nc.Dataset(nc_file)\n",
    "        variable_names = list(data.variables.keys())\n",
    "        latitude = data.variables[variable_names[2]][:]\n",
    "        longitude = data.variables[variable_names[1]][:]\n",
    "        time_var = data.variables['time']\n",
    "        dates = num2date(time_var[:], units=time_var.units, calendar=time_var.calendar)\n",
    "        if len(variable_names) == 5:\n",
    "            variable = data.variables[variable_names[4]][:, :, :, :]\n",
    "            variable = variable[:, 0, :, :]\n",
    "        else:\n",
    "            variable = data.variables[variable_names[3]][:, :, :]\n",
    "        data_ = np.array(variable)\n",
    "        data_[data_ == -9.96921e+36] = 0\n",
    "        data_ = data_[:-12,:,:]\n",
    "        data_reshaped = np.reshape(data_, (len(data_) // 12, 12, data_.shape[1], data_.shape[2]))\n",
    "        return data_reshaped, latitude, longitude\n",
    "\n",
    "    def lon_pp(self, longitude):\n",
    "        lon_indices = []\n",
    "        for start_value in np.arange(0, 360, 20):\n",
    "            end_value = start_value + 20\n",
    "            range_indices = np.where((longitude >= start_value) & (longitude < end_value))[0]\n",
    "            if len(range_indices) > 0:\n",
    "                min_index = range_indices[np.argmin(longitude[range_indices])]\n",
    "                max_index = range_indices[np.argmax(longitude[range_indices])]\n",
    "                lon_indices.append(min_index)\n",
    "                lon_indices.append(max_index)\n",
    "        return lon_indices\n",
    "\n",
    "    def lat_pp(self, latitude):\n",
    "        lat_indices = []\n",
    "        for start_value in np.arange(90, -90, -10):\n",
    "            end_value = start_value - 10\n",
    "            range_indices = np.where((latitude <= start_value) & (latitude > end_value))[0]\n",
    "            if len(range_indices) > 0:\n",
    "                min_index = range_indices[np.argmax(latitude[range_indices])]\n",
    "                max_index = range_indices[np.argmin(latitude[range_indices])]\n",
    "                lat_indices.append(min_index)\n",
    "                lat_indices.append(max_index)\n",
    "        return lat_indices\n",
    "\n",
    "    def coarse_gridding(self, data, lat_indices, lon_indices):\n",
    "        coarsed_data = np.zeros((data.shape[0], data.shape[1], len(lat_indices) // 2, len(lon_indices) // 2))\n",
    "        for lat_idx in range(0, len(lat_indices), 2):\n",
    "            lat_range_start = lat_indices[lat_idx]\n",
    "            lat_range_end = lat_indices[lat_idx + 1]\n",
    "            for lon_idx in range(0, len(lon_indices), 2):\n",
    "                lon_range_start = lon_indices[lon_idx]\n",
    "                lon_range_end = lon_indices[lon_idx + 1]\n",
    "                subset = data[:, :, lat_range_start:lat_range_end, lon_range_start:lon_range_end]\n",
    "                averaged_value = np.mean(subset, axis=(2, 3))\n",
    "                coarsed_data[:, :, lat_idx // 2, lon_idx // 2] = averaged_value\n",
    "        final = np.reshape(coarsed_data, (len(coarsed_data), 12, coarsed_data.shape[2]*coarsed_data.shape[3]))\n",
    "        return final\n",
    "\n",
    "    def calculate_monthly_anomalies(self, data):\n",
    "        anomalies = np.zeros((data.shape[0], data.shape[1], data.shape[2]))\n",
    "        means = np.zeros((data.shape[1], data.shape[2]))\n",
    "        for i in range(data.shape[2]):\n",
    "            for j in range(data.shape[1]):\n",
    "                month = data[:, j, i]\n",
    "                monthly_mean = np.mean(month)\n",
    "                means[j, i] = monthly_mean\n",
    "                anomalies[:, j, i] = month - monthly_mean\n",
    "                \n",
    "\n",
    "        return means,anomalies\n",
    "    \n",
    "\n",
    "    def execute_pipeline(self, nc_file1, nc_file2):\n",
    "        data_reshaped1, latitude1, longitude1 = self.preprocessing(nc_file1)\n",
    "        data_reshaped2, latitude2, longitude2 = self.preprocessing(nc_file2)\n",
    "        \n",
    "        lon_indices1 = self.lon_pp(longitude1)\n",
    "        lat_indices1 = self.lat_pp(latitude1)\n",
    "        lon_indices2 = self.lon_pp(longitude2)\n",
    "        lat_indices2 = self.lat_pp(latitude2)\n",
    "\n",
    "        coarsed_data1 = self.coarse_gridding(data_reshaped1, lat_indices1, lon_indices1)\n",
    "        coarsed_data2 = self.coarse_gridding(data_reshaped2, lat_indices2, lon_indices2)\n",
    "\n",
    "        monthly_means1,anomalies1 = self.calculate_monthly_anomalies(coarsed_data1)\n",
    "        monthly_means2,anomalies2 = self.calculate_monthly_anomalies(coarsed_data2)\n",
    "        \n",
    "        monthly_means1= monthly_means1.reshape(1,12,324)\n",
    "        monthly_means2= monthly_means2.reshape(1,12,324)\n",
    "\n",
    "        # Concatenate anomalies along the appropriate axis (assuming anomalies have the same shape)\n",
    "        concatenated_anomalies = np.concatenate((anomalies1, anomalies2), axis=2)\n",
    "        concatenated_means = np.concatenate((monthly_means1, monthly_means2), axis=2)\n",
    "\n",
    "        #normalized_data = self.min_max_normalize(concatenated_anomalies)\n",
    "        return concatenated_means,concatenated_anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0036ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = DataPreprocessor()\n",
    "means, anomalies = processor.execute_pipeline(\"AIRT2023.nc\", \"GPH200.nc\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24593aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('long_term_mean.pkl', 'wb') as f:\n",
    "    pickle.dump(means, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0ad0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('long_term_mean.pkl', 'rb') as f:\n",
    "    long_term_mean = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa20feeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('long_term_anomalies.pkl', 'wb') as f:\n",
    "    pickle.dump(anomalies, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68c12b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('long_term_anomalies.pkl', 'rb') as f:\n",
    "    long_term_anomalies = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
